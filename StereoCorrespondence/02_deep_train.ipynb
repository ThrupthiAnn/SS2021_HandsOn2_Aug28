{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Stereo/02_deep_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I8bVmIWlLkp"
   },
   "source": [
    "In the previous section, we have seen stereo matching using traditional shallow features with different metrics. \n",
    "In this section, we will see how to train deep network for stereo matching problem. \n",
    "\n",
    "The basic idea is to train a deep network that can perform patch matching. To train the network, we will use KITTI 2015 dataset available at http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo.\n",
    "\n",
    "The dataset provides a pair of stero-images with ground-truth disparity between the images. Using this dataset, \n",
    "we will create pairs of (left, right, disparity) data and then train a network that predicts the disparity given a pair of images.\n",
    "\n",
    "This tutorial is primarily based on \"Wenjie Luo et al. Efficient Deep Learning for Stereo Matching\". Interested readers can refer the above paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86d3sIU0lLku",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po1KDckNlLky",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "root = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S32czT49lTAe"
   },
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=13G6FvNhz8AZQwrPVjnraKvjxOoQjEjrv' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=13G6FvNhz8AZQwrPVjnraKvjxOoQjEjrv\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydmLkftklLk2"
   },
   "source": [
    "Let's look into some of the KITTI images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThakZMHQlLk5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(join(root, \"000123_11_L.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlM5IeYKlLk7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(join(root, \"000123_11_R.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWBa_gAslLk9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.open(join(root, \"000123_disp.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_hetIOIlLlB"
   },
   "source": [
    "### Patch Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cofOdeZvlLlE"
   },
   "source": [
    "We need to extract patches from these left and right images, and train a network with a known disparity between them.\n",
    "\n",
    "We extract patches of size 37x37 from left images. From the corresponding right images, we extract larger patches of size 37x237, centered around disparity location given in the disparity map.\n",
    "\n",
    "For e.g. if the left patch in the left image is centered at (x1,y1) has a dimension of 37x37. Let's assume the ground truth disparity at (x1,y1) in disparity map is \"d\".  We then extract a right patch centered at (x1+d, y1) in the right image with a dimension of 37x237."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D81IHPOclLlH"
   },
   "source": [
    "Lets look into some of the left and right patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jUOEALXlLlI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "left_patches = torch.tensor(np.load(join(root, \"left_patches.npy\")))\n",
    "right_patches = torch.tensor(np.load(join(root, \"right_patches.npy\")))\n",
    "targets = torch.tensor(np.load(join(root, \"targets.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pqVKAUtlLlK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(left_patches.size())\n",
    "print(right_patches.size())\n",
    "print(targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXsuzS1tlLlL"
   },
   "source": [
    "Lets display some of these images.. You may notice some artifacts as we normalized the patches with mean and standard deviation of the images. \n",
    "\n",
    "The disparity value specifies the position at which left patch is occuring in the larger right patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4Un7QUjlLlN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Left patch..\")\n",
    "plt.imshow(transforms.ToPILImage()(left_patches[2000]))\n",
    "plt.show()\n",
    "print(\"right patch..\")\n",
    "plt.imshow(transforms.ToPILImage()(right_patches[2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0SL1VTwlLlP"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBlKpWHslLlQ"
   },
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_vKiyUMlLlR"
   },
   "source": [
    "Let's create a deep network with 9 conv-batchnorm-relu layers. We use filters of constant size 5x5. Our architecture is a siamese network with shared weights between them.\n",
    "\n",
    "<img src=\"https://github.com/Ektagavas/CVSummerSchool2021/blob/main/Stereo/images/siamese.png?raw=1\" width=400>\n",
    "We basically pass smaller left and larger right patches through the network and obtain the 64-dimensional deep features. The similarities are then computed using deep features to obtain the similarity score across multiple locations in the larger patch. The network is then optimized such that larger correlation is obtained at location given by the disparity (displacement) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmSr62STlLlS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nChannel, max_dips):\n",
    "        super(Net, self).__init__()\n",
    "        self.l_max_dips = max_dips\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            nChannel, 32, 5\n",
    "        )  # first conv layer: 32 filters of size 5x5\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32, 1e-3)  # first batch normalization layer\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5)  # second conv layer: 32 filters of size 5x5\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, 1e-3)  # second normalization layer\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)  # third conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64, 1e-3)  # third batch normalization layer\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5)  # fourth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm4 = nn.BatchNorm2d(64, 1e-3)  # fourth batch normalization layer\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 64, 5)  # fifth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm5 = nn.BatchNorm2d(64, 1e-3)  # fifth batch normalization layer\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 64, 5)  # sixth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm6 = nn.BatchNorm2d(64, 1e-3)  # sixth batch normalization layer\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 64, 5)  # seventh conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm7 = nn.BatchNorm2d(64, 1e-3)  # seventh batch normalization layer\n",
    "\n",
    "        self.conv8 = nn.Conv2d(64, 64, 5)  # eighth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm8 = nn.BatchNorm2d(64, 1e-3)  # eigth batch normalization layer\n",
    "\n",
    "        self.conv9 = nn.Conv2d(64, 64, 5)  # ninth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm9 = nn.BatchNorm2d(64, 1e-3)  # ninth batch normalization layer\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.batchnorm6(x))\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.batchnorm7(x))\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.batchnorm8(x))\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.batchnorm9(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # forward pass left patch of 37x37\n",
    "        x1 = self.forward_pass(x1)\n",
    "        # forward pass right patch of 37*237\n",
    "        x2 = self.forward_pass(x2)\n",
    "\n",
    "        # left patch feature vector (1,64) dimension\n",
    "        x1 = x1.view(x1.size(0), 1, 64)\n",
    "\n",
    "        # right patch feature matrix (64, 201) dimension\n",
    "        # at 201 locations\n",
    "        x2 = x2.squeeze().view(x2.size(0), 64, self.l_max_dips)\n",
    "\n",
    "        # multiply the features to get correlation at 201 location\n",
    "        x3 = x1.bmm(x2).view(x2.size(0), self.l_max_dips)\n",
    "\n",
    "        # compute log p_i(y_i) of scores\n",
    "        x3 = self.logsoftmax(x3)\n",
    "\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DWPHsIrlLlU"
   },
   "source": [
    "Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lUAEhPolLlV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maxmium number of epochs to train the model\n",
    "max_epoch = 5\n",
    "\n",
    "# number of iterations in each epoch\n",
    "iter_per_epoch = 50\n",
    "\n",
    "# number of samples in each iteration\n",
    "batchSize = 64\n",
    "\n",
    "# gpu option. set 1 if available, else 0\n",
    "gpu = 1\n",
    "\n",
    "# learning rate used for Adam optimizer\n",
    "learn_rate = 0.01\n",
    "\n",
    "# momentum parameter for Adam\n",
    "momentum = 0.9\n",
    "\n",
    "# weight decay\n",
    "weightDecay = 0.0005\n",
    "\n",
    "# left patch is searched for 2*half_range + 1 locations\n",
    "# in the right patch.\n",
    "\n",
    "# If left patch is 37*37, and we consider\n",
    "# right patch of size 37 * (37+2*half_range)\n",
    "half_range = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ1OzsuZlLlW"
   },
   "source": [
    "## Custom Loss function      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jP1rXk-lLlX"
   },
   "source": [
    "We minimize cross-entropy loss with respect to the weights w that parameterize the network\n",
    "\n",
    "<img src='https://github.com/Ektagavas/CVSummerSchool2021/blob/main/Stereo/images/eq.png?raw=1' width=\"300\">\n",
    "where the class weights are given as\n",
    "<img src='https://github.com/Ektagavas/CVSummerSchool2021/blob/main/Stereo/images/eq2.png?raw=1' width=\"300\">\n",
    "\n",
    "This basically implies that scores at desired location (given by disparity target ground truth) should be larger compared to other locations. To handle noise, two similarity scores on either side of disparity location are considered, however with smaller weightage given to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEOhpnKblLlX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# three pixel error\n",
    "def loss_function(x3, t, w):\n",
    "    error = 0\n",
    "    for i in range(x3.size(0)):\n",
    "        # scores at ground truth target locations.\n",
    "        # instead of taking single score at exact target disparity\n",
    "        # location two more locations on either side are considered\n",
    "        # with less weightage.\n",
    "        sc = x3[i, t[i][0] - 2 : t[i][0] + 2 + 1]\n",
    "\n",
    "        # class_weight_y_i* log p_i(y_i)\n",
    "        loss_sample = torch.mul(sc, w).sum()\n",
    "\n",
    "        error = error - loss_sample\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxsKQVnYlLlY"
   },
   "source": [
    "We define the class weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq-sL9nXlLlZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_wts = torch.Tensor([1, 4, 10, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAE0YSO6lLlZ"
   },
   "outputs": [],
   "source": [
    "# instantiate a model\n",
    "model = Net(3, half_range * 2 + 1)\n",
    "if gpu:\n",
    "    model = model.to(device)\n",
    "    class_wts = class_wts.to(device)\n",
    "model.train()  # train mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ccikBHlLla"
   },
   "source": [
    "We use Adam optimizer for training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLFz736KlLla",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=learn_rate, eps=1e-08, weight_decay=weightDecay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In_Qn6bilLlb"
   },
   "source": [
    "Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QrZqnCvlLlb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    train_loss = 0\n",
    "    for _iter in range(iter_per_epoch):\n",
    "\n",
    "        # zero the gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # sample batch data\n",
    "        id1 = epoch * iter_per_epoch * batchSize + _iter * batchSize\n",
    "        id2 = epoch * iter_per_epoch * batchSize + (_iter + 1) * batchSize\n",
    "        left_batch = left_patches[id1:id2, :, :, :]\n",
    "        right_batch = right_patches[id1:id2, :, :, :]\n",
    "        t_batch = targets[id1:id2].view(batchSize, 1).int()\n",
    "\n",
    "        # convert to cuda if gpu available\n",
    "        if gpu:\n",
    "            left_batch = left_batch.to(device)\n",
    "            right_batch = right_batch.to(device)\n",
    "            t_batch = t_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        x1, x2, x3 = model(left_batch, right_batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_function(x3, t_batch, class_wts)\n",
    "\n",
    "        # backward pass. compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(\"Loss at epoch\", epoch, train_loss / iter_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "02_deep_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
