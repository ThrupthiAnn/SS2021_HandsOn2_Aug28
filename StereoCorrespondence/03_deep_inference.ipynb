{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Stereo/03_deep_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPUaWCg4oTGo"
   },
   "source": [
    "In this section, given a trained deep model from the previous section, we run it on pair of stereo images and obtain the disparity map between them. The basic idea is to pass the pair of images through the network optimised for disparity estimation. The disparity is obtained by computing the similarities betwen the images in deep feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUutPoE9oTGt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Tqkf-wYoWXf"
   },
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=13G6FvNhz8AZQwrPVjnraKvjxOoQjEjrv' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=13G6FvNhz8AZQwrPVjnraKvjxOoQjEjrv\" -O data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYUjj7jgoTGx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "root = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZnrYWicoTG2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trained model file (from previous section)\n",
    "model_fn = join(root, \"model.t7\")\n",
    "\n",
    "# running mean and variances of batch norm layers\n",
    "bn_running_mean_fn = join(root, \"bn_mean.pkl\")\n",
    "bn_running_var_fn = join(root, \"bn_var.pkl\")\n",
    "\n",
    "# colour images\n",
    "nChannel = 3\n",
    "\n",
    "# search range\n",
    "disp_range = 128\n",
    "\n",
    "# fixed seed for random variable generation.\n",
    "# so that we get the same result when we run\n",
    "# multiple times\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3p9QX18oTG5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nChannel):\n",
    "        super(Net, self).__init__()\n",
    "        self.pad = nn.ReflectionPad2d(\n",
    "            18\n",
    "        )  # perform 18 pixel padding on the image on all sides.\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            nChannel, 32, 5\n",
    "        )  # first conv layer: 32 filters of size 5x5\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32, 1e-3)  # first batch normalization layer\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5)  # second conv layer: 32 filters of size 5x5\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32, 1e-3)  # second normalization layer\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5)  # third conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64, 1e-3)  # third batch normalization layer\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5)  # fourth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm4 = nn.BatchNorm2d(64, 1e-3)  # fourth batch normalization layer\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 64, 5)  # fifth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm5 = nn.BatchNorm2d(64, 1e-3)  # fifth batch normalization layer\n",
    "\n",
    "        self.conv6 = nn.Conv2d(64, 64, 5)  # sixth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm6 = nn.BatchNorm2d(64, 1e-3)  # sixth batch normalization layer\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 64, 5)  # seventh conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm7 = nn.BatchNorm2d(64, 1e-3)  # seventh batch normalization layer\n",
    "\n",
    "        self.conv8 = nn.Conv2d(64, 64, 5)  # eighth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm8 = nn.BatchNorm2d(64, 1e-3)  # eigth batch normalization layer\n",
    "\n",
    "        self.conv9 = nn.Conv2d(64, 64, 5)  # ninth conv layer: 64 filters of size 5x5\n",
    "        self.batchnorm9 = nn.BatchNorm2d(64, 1e-3)  # ninth batch normalization layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pad(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.batchnorm6(x))\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.batchnorm7(x))\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.batchnorm8(x))\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.batchnorm9(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IyvGy_poTG9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the trained model\n",
    "net = Net(nChannel)\n",
    "net.load_state_dict(torch.load(model_fn))\n",
    "net.eval()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7Mqnd9-oTHB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTNNQJNjoTHD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set batch norm running mean and variance parameters.. this is important when using batch normalization layer\n",
    "bn_mean, bn_var = torch.load(bn_running_mean_fn), torch.load(bn_running_var_fn)\n",
    "\n",
    "for i in range(9):\n",
    "    bn_mean[i], bn_var[i] = bn_mean[i].to(device), bn_var[i].to(device)\n",
    "\n",
    "net.batchnorm1.running_mean, net.batchnorm1.running_var = bn_mean[0], bn_var[0]\n",
    "net.batchnorm2.running_mean, net.batchnorm2.running_var = bn_mean[1], bn_var[1]\n",
    "net.batchnorm3.running_mean, net.batchnorm3.running_var = bn_mean[2], bn_var[2]\n",
    "net.batchnorm4.running_mean, net.batchnorm4.running_var = bn_mean[3], bn_var[3]\n",
    "net.batchnorm5.running_mean, net.batchnorm5.running_var = bn_mean[4], bn_var[4]\n",
    "net.batchnorm6.running_mean, net.batchnorm6.running_var = bn_mean[5], bn_var[5]\n",
    "net.batchnorm7.running_mean, net.batchnorm7.running_var = bn_mean[6], bn_var[6]\n",
    "net.batchnorm8.running_mean, net.batchnorm8.running_var = bn_mean[7], bn_var[7]\n",
    "net.batchnorm9.running_mean, net.batchnorm9.running_var = bn_mean[8], bn_var[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmt2FQhsoTHG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some functions to dispay images and disparity maps in ipython notebook\n",
    "\n",
    "# convert disparity to colour image\n",
    "def disparity_to_color(I):\n",
    "    _map = np.array(\n",
    "        [\n",
    "            [0, 0, 0, 114],\n",
    "            [0, 0, 1, 185],\n",
    "            [1, 0, 0, 114],\n",
    "            [1, 0, 1, 174],\n",
    "            [0, 1, 0, 114],\n",
    "            [0, 1, 1, 185],\n",
    "            [1, 1, 0, 114],\n",
    "            [1, 1, 1, 0],\n",
    "        ]\n",
    "    )\n",
    "    max_disp = 1.0 * I.max()\n",
    "    I = np.minimum(I / max_disp, np.ones_like(I))\n",
    "\n",
    "    A = I.transpose()\n",
    "    num_A = A.shape[0] * A.shape[1]\n",
    "\n",
    "    bins = _map[0 : _map.shape[0] - 1, 3]\n",
    "    cbins = np.cumsum(bins)\n",
    "    cbins_end = cbins[-1]\n",
    "    bins = bins / (1.0 * cbins_end)\n",
    "    cbins = cbins[0 : len(cbins) - 1] / (1.0 * cbins_end)\n",
    "\n",
    "    A = A.reshape(1, num_A)\n",
    "    B = np.tile(A, (6, 1))\n",
    "    C = np.tile(np.array(cbins).reshape(-1, 1), (1, num_A))\n",
    "\n",
    "    ind = np.minimum(sum(B > C), 6)\n",
    "    bins = 1 / bins\n",
    "    cbins = np.insert(cbins, 0, 0)\n",
    "\n",
    "    A = np.multiply(A - cbins[ind], bins[ind])\n",
    "    K1 = np.multiply(_map[ind, 0:3], np.tile(1 - A, (3, 1)).T)\n",
    "    K2 = np.multiply(_map[ind + 1, 0:3], np.tile(A, (3, 1)).T)\n",
    "    K3 = np.minimum(np.maximum(K1 + K2, 0), 1)\n",
    "\n",
    "    return np.reshape(K3, (I.shape[1], I.shape[0], 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kytY46cYoTHJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# left image\n",
    "left_image_fn = join(root, \"kitti2015_000000_10_L.png\")\n",
    "# left_image_fn = '../../data/lab6/imgL.png'\n",
    "\n",
    "# right image\n",
    "right_image_fn = join(root, \"kitti2015_000000_10_R.png\")\n",
    "# right_image_fn = '../../data/lab6/imgR.png'\n",
    "\n",
    "# display left and right images\n",
    "plt.imshow(Image.open(left_image_fn))\n",
    "plt.show()\n",
    "plt.imshow(Image.open(right_image_fn))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# read images into tensor\n",
    "l_img = 255 * transforms.ToTensor()(Image.open(left_image_fn))\n",
    "r_img = 255 * transforms.ToTensor()(Image.open(right_image_fn))\n",
    "\n",
    "# normalize images. All the patches used for training were normalized.\n",
    "l_img = (l_img - l_img.mean()) / (l_img.std())\n",
    "r_img = (r_img - r_img.mean()) / (r_img.std())\n",
    "\n",
    "img_h = l_img.size(1)\n",
    "img_w = l_img.size(2)\n",
    "print(\"Image size:\", img_h, img_w)\n",
    "\n",
    "# convert to batch x channel x height x width format\n",
    "l_img = l_img.view(1, l_img.size(0), l_img.size(1), l_img.size(2))\n",
    "r_img = r_img.view(1, r_img.size(0), r_img.size(1), r_img.size(2))\n",
    "\n",
    "l_img = l_img.to(device)\n",
    "r_img = r_img.to(device)\n",
    "\n",
    "# forward pass. extract deep features\n",
    "# forward pass left image\n",
    "with torch.no_grad():\n",
    "    left_feat = net(l_img)\n",
    "    # forward pass right image\n",
    "    right_feat = net(r_img)\n",
    "\n",
    "# output tensor\n",
    "output = torch.Tensor(img_h, img_w, disp_range).zero_()\n",
    "output = output.to(device)\n",
    "\n",
    "# correlation in deep feature space\n",
    "# we slide the left image over right image for \"disp_range\" locations\n",
    "# in x-direction and then correlate the features in the overlapping window\n",
    "end_id = img_w - 1\n",
    "for loc_idx in range(disp_range):\n",
    "    # 1 x 64 x height x (overlap in x-direction after sliding by loc_idx)\n",
    "    # overlapping region loc_idx:img_w-1\n",
    "    l = left_feat[:, :, :, loc_idx:end_id]\n",
    "    # 1 x 64 x height x (overlap in x-direction after sliding by loc_idx)\n",
    "    # overlap region 0:end_id-loc_idx\n",
    "    r = right_feat[:, :, :, 0 : end_id - loc_idx]\n",
    "    p = torch.mul(l, r)  # elementwise multiply 64 dimensional feature\n",
    "    x = torch.sum(p, 1)  # and add\n",
    "    output[:, loc_idx:end_id, loc_idx] = x.squeeze()\n",
    "\n",
    "# find the location with maximum score at each pixel in the 3D output (height x width x 64)\n",
    "max_disp, pred = torch.max(output, 2)\n",
    "\n",
    "# disparity map (height x width)\n",
    "pred = pred.view(output.size(0), output.size(1))\n",
    "\n",
    "# convert to colour\n",
    "color_map = disparity_to_color(pred.float().cpu().numpy())\n",
    "# showarray(255*np.transpose(color_map, axes=[1,2,0]))\n",
    "plt.imshow(np.transpose(color_map, axes=[1, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2n1cI0UoTHM"
   },
   "source": [
    "Final note: Its a common practice to apply post-processing to smooth the images. We are not considering the output smoothing in this tutorial. Interested readers can refer to \"[Wenjie Luo et al. Efficient Deep Learning for Stereo Matching](https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf)\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "03_deep_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
